# Desarrollo del Examen Final- Julio Cesar Alvarez Casas

## 1. Justificaci√≥n del Uso de la Nube ‚Äì Google Cloud Platform (GCP)

### 1.1 Contexto del Proyecto BI

El presente proyecto consiste en la construcci√≥n en vivo de una soluci√≥n completa de Business Intelligence en la nube, partiendo de uno o m√°s archivos en formato CSV proporcionados durante el examen.  
El alcance de esta fase (BRONCE) contempla:

- Ingesti√≥n de datos sin modificar (raw).
- Estructuraci√≥n b√°sica y validaciones iniciales.
- Exploraci√≥n inicial de los datos (EDA m√≠nimo).
- Preparaci√≥n para una arquitectura Medallion escalable.

Dado que el dataset puede variar en estructura, tama√±o y dominio, la nube seleccionada debe ofrecer flexibilidad, rapidez de despliegue, bajo overhead operativo y costos m√≠nimos, priorizando servicios serverless.

---

### 1.2 Criterios de Evaluaci√≥n de la Nube

La selecci√≥n de la plataforma cloud se bas√≥ en los siguientes criterios t√©cnicos, aplicables a cualquier tipo de CSV:

- Soporte nativo para almacenamiento de archivos CSV.
- Escalabilidad autom√°tica para cargas batch.
- Modelo serverless (sin gesti√≥n de infraestructura).
- Bajo costo para ejecuciones espor√°dicas.
- Seguridad por defecto y trazabilidad.
- Integraci√≥n directa con Python para an√°lisis exploratorio.
- Compatibilidad con arquitectura Medallion (Bronce‚ÄìPlata‚ÄìOro).

---

### 1.3 Comparaci√≥n General: AWS vs Azure vs GCP

| Criterio | AWS | Azure | GCP |
|--------|-----|-------|-----|
| Almacenamiento de CSV | S3 (requiere configuraci√≥n manual de seguridad) | ADLS Gen2 | Cloud Storage (seguridad por defecto) |
| Procesamiento batch | EMR / Glue (basado en cl√∫steres) | Synapse Spark Pools | Dataproc Serverless |
| Gesti√≥n de infraestructura | Media‚ÄìAlta | Media | M√≠nima |
| Escalamiento autom√°tico | Parcial | Parcial | Completo |
| Costos para uso acad√©mico | Moderados | Altos | Muy bajos / Free Tier |
| Integraci√≥n con Python | Buena | Buena | Nativa y directa |
| Curva de aprendizaje | Media | Media‚ÄìAlta | Baja |

---

### 1.4 Ventajas Clave de GCP para Ingesti√≥n de CSV (Capa Bronce)

Google Cloud Platform resulta especialmente adecuada para escenarios donde:

- El dataset es desconocido hasta el momento del examen.
- Se requiere cargar archivos CSV r√°pidamente mediante CLI.
- Se necesita ejecutar scripts Python de EDA sin aprovisionar cl√∫steres.
- Las ejecuciones son puntuales o de baja frecuencia.

Ventajas t√©cnicas principales:

- Cloud Storage permite almacenar archivos CSV sin esquema previo.
- Cifrado autom√°tico y auditor√≠a habilitados por defecto.
- Dataproc Serverless permite ejecutar procesamiento bajo demanda.
- Facturaci√≥n por uso real, evitando costos fijos.
- Ecosistema integrado que facilita la evoluci√≥n hacia capas Plata y Oro.

---

### 1.5 Adecuaci√≥n a Arquitectura Medallion

GCP se adapta de forma natural a una arquitectura Medallion:

```bash
/bronce
‚îú‚îÄ‚îÄ raw ‚Üí CSV originales cargados v√≠a CLI
‚îú‚îÄ‚îÄ processed ‚Üí CSV con validaciones b√°sicas y normalizaci√≥n inicial
‚îî‚îÄ‚îÄ curated ‚Üí Datos listos para consumo anal√≠tico inicial
```

- Cloud Storage act√∫a como Data Lake en la capa Bronce.
- No se asume un esquema fijo; los CSV pueden variar en columnas y tipos.
- El procesamiento inicial se realiza mediante scripts Python, permitiendo:
  - Detecci√≥n de valores nulos.
  - Identificaci√≥n de tipos de datos.
  - Estad√≠sticos descriptivos b√°sicos.
  - Validaciones estructurales m√≠nimas.

---

### 1.6 Decisi√≥n Final

Se seleccion√≥ Google Cloud Platform (GCP) porque ofrece el mejor equilibrio entre simplicidad operativa, escalabilidad, seguridad y costos para una soluci√≥n BI basada en archivos CSV de estructura variable.  
GCP permite implementar la capa BRONCE de forma r√°pida, reproducible y sin gesti√≥n de infraestructura, lo cual resulta ideal para un desarrollo en vivo y para una posterior evoluci√≥n hacia una arquitectura anal√≠tica completa.
## CREACI√ìN DEL PROYECTO
## Pre-requisitos

### Instalar Google Cloud SDK (gcloud CLI)

**En Windows:**
```powershell
# Descargar e instalar desde:
# https://cloud.google.com/sdk/docs/install

# Verificar instalaci√≥n
gcloud --version
```

**En macOS/Linux:**
```bash
# Descargar instalador
curl https://sdk.cloud.google.com | bash

# Reiniciar terminal y verificar
gcloud --version
```

### 2. Autenticarse en GCP

```bash
# Iniciar sesi√≥n con tu cuenta de Google
gcloud auth login

# Esto abrir√° tu navegador para autenticaci√≥n
# Sigue los pasos en el navegador
```
![1](Evidencias/1-Loging.png)

## Inicio R√°pido
### 1. Estructura local requerida

Antes de ejecutar el despliegue, se debe contar con la siguiente estructura local:
```bash
.
‚îú‚îÄ‚îÄ deploy.sh
‚îú‚îÄ‚îÄ destroy.sh
‚îî‚îÄ‚îÄ csv/
    ‚îú‚îÄ‚îÄ archivo1.csv
    ‚îú‚îÄ‚îÄ archivo2.csv
    ‚îî‚îÄ‚îÄ ...
```

La carpeta csv/ contendr√° todos los archivos CSV proporcionados en el examen.

### 2. Asignar permisos de ejecuci√≥n
```bash
chmod +x deploy.sh destroy.sh
```
### 3. Despliegue autom√°tico de la Capa Bronce
```bash
./deploy.sh
```

Este script realiza autom√°ticamente:

- Creaci√≥n del proyecto en GCP.

- Creaci√≥n del bucket de almacenamiento.

- Generaci√≥n de la estructura Bronce.

- Carga de archivos CSV v√≠a CLI.

- Creaci√≥n de un cl√∫ster Dataproc para EDA inicial.

### Verificaci√≥n de Resultados
Listar archivos cargados en Bronce (raw)
```bash
gsutil ls gs://examen-final-julio-cesar-bucket/bronce/raw/
```

### Uso del Cl√∫ster para EDA

El cl√∫ster Dataproc incluye Jupyter Notebook, accesible desde la consola de GCP mediante el Component Gateway.

Este cl√∫ster permite realizar un EDA inicial en Python o PySpark, incluyendo:

- Lectura directa de CSV desde Cloud Storage.

- An√°lisis de tipos de datos.

- Estad√≠sticos descriptivos b√°sicos.

- Identificaci√≥n de valores nulos y duplicados.


### CREAR CLUSTER DATAPROC PARA EDA

```bash
gcloud dataproc clusters create practica-final-julio-alvarez-2 --enable-component-gateway --bucket practica-final-julio-alvarez-bucket-2 --region us-east1 --no-address --master-machine-type n1-standard-2 --master-boot-disk-size 100 --num-workers 2 --worker-machine-type n1-standard-2 --worker-boot-disk-size 200 --image-version 2.2-debian12 --optional-components JUPYTER,ZOOKEEPER --max-idle 7200s --max-age 7200s --scopes 'https://www.googleapis.com/auth/cloud-platform' --project practica-final-julio-alvarez-2
```


### Observaci√≥n Final

La infraestructura creada corresponde exclusivamente a la Capa Bronce, asegurando ingesti√≥n reproducible, estructuraci√≥n m√≠nima y an√°lisis exploratorio inicial, sentando las bases para la posterior evoluci√≥n hacia Plata y Oro.

# üìì EDA ‚Äì Notebook Jupyter (Dataproc)

Objetivo: an√°lisis exploratorio robusto sobre archivo1.csv, sin asumir esquema previo.

## 1. Importaci√≥n de librer√≠as
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

pd.set_option("display.max_columns", None)
plt.style.use("default")
```

## 2. Carga del dataset desde Cloud Storage
### Ruta del archivo en Cloud Storage

```python
GCS_PATH = "gs://examen-final-julio-cesar-bucket/bronce/raw/archivo1.csv"

### Lectura gen√©rica del CSV
df = pd.read_csv(GCS_PATH)

### Vista inicial
df.head()
```

## 3. Estructura general del dataset
# Dimensiones del dataset
```python
print("Filas, Columnas:", df.shape)

# Tipos de datos
df.dtypes
```
## 4. An√°lisis de valores nulos
```python
# Conteo de nulos por columna

nulls = df.isnull().sum()

### Porcentaje de nulos
nulls_pct = (nulls / len(df)) * 100

nulls_df = pd.DataFrame({
    "nulos": nulls,
    "porcentaje (%)": nulls_pct
}).sort_values(by="porcentaje (%)", ascending=False)

nulls_df

Visualizaci√≥n de nulos:

plt.figure(figsize=(10, 4))
sns.heatmap(df.isnull(), cbar=False)
plt.title("Mapa de valores nulos")
plt.show()
```

## 5. Estad√≠sticas descriptivas
```python
# Estad√≠sticas para variables num√©ricas
df.describe()

#Para variables categ√≥ricas:

df.describe(include=["object"])
```
## 6. Distribuci√≥n de variables num√©ricas
```python
numeric_cols = df.select_dtypes(include=np.number).columns

df[numeric_cols].hist(
    figsize=(14, 8),
    bins=20,
    edgecolor="black"
)

plt.suptitle("Distribuci√≥n de variables num√©ricas")
plt.show()
```
## 7. Detecci√≥n de valores at√≠picos (Outliers)

```python
for col in numeric_cols:
    plt.figure(figsize=(5, 3))
    sns.boxplot(x=df[col])
    plt.title(f"Boxplot - {col}")
    plt.show()
```

## 8. Correlaciones (si existen variables num√©ricas)

```python
if len(numeric_cols) >= 2:
    corr = df[numeric_cols].corr()

    plt.figure(figsize=(10, 6))
    sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f")
    plt.title("Matriz de correlaci√≥n")
    plt.show()
else:
    print("No hay suficientes variables num√©ricas para correlaci√≥n.")
```

## 9. Duplicados
```python
# Conteo de filas duplicadas
duplicados = df.duplicated().sum()
print("Filas duplicadas:", duplicados)
```